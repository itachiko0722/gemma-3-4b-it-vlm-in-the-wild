{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKOmOxizJKOz",
    "outputId": "655118e0-b905-470d-d4f5-a98f79421dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198,
     "referenced_widgets": [
      "dad932a9fb14468187b05d030b381d40",
      "dafa848d595948269344813ae606019f",
      "5aae52cd63ff4541ba193f2737cc1692",
      "48bda141b4ee4efb8314bd418dcf7995",
      "d0e2a9d0419d457bb2796191dae10629",
      "3dfab0ee3d3442d5b83d99bdbf548b45",
      "2797c510aa534753a00ac978e721c18a",
      "c8f87827437c4e0db0da258714a4c20d",
      "6368dac1147b411bbe50d4039d5a8954",
      "f436543f85a74d9daba22177ccebeda6",
      "ae927396182045dc93da4bc888442c8e"
     ]
    },
    "id": "O3pAGl9aJOg2",
    "outputId": "fe4d40db-565f-4f36-9dd3-133c2501b319"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12af1ef0a89544d6841f674560e80b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b8598925824538bd48e87e70f86571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6da55271c7646bb900291c78e0c41c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ca06e8095641119709a0657a8e28fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b97c6071b814b6c8e0c908b1ae912c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dc72287e354fa9ace8eb4226cd9725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bc6fbbdfad45cd92bbc5f38899f60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0a419c8a2f4b2c81fe95cb211f275a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6e96a1e4654f0aa3079cf053b35193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b56ddc88d11459aad55d352892b53f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b15a325c7df49f9a9c68ee378cd2f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d76c81db3144809088b24ddad3c9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcaf7ce9b0e41dca00f680293c8d49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77313df663a545bdb6d0ce60b17eaacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a6a6a65e5248e99180a058140cf5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gyHvhp-AI5ss"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Image as dsImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "example_output = {\n",
    "    \"image_id\": 100,\n",
    "    \"qa_id\": 10674688,\n",
    "    \"url\": \"https://example.com/image.jpg\",\n",
    "    \"question\": \"どんな天候ですか？\",\n",
    "    \"pred_answer\": \"晴れの日\",\n",
    "    \"gt_answer\": \"晴天\"\n",
    "}\n",
    "\n",
    "\n",
    "print(json.dumps(example_output, ensure_ascii=False, indent=2))\n",
    "\n",
    "if 'ds' in locals():\n",
    "        first_example = next(iter(ds[\"test\"]))\n",
    "        print(\"\\n実際のデータセット例:\")\n",
    "        print(f\"質問: {first_example['question']}\")\n",
    "        print(f\"回答: {first_example['answer']}\")\n",
    "        \n",
    "        \n",
    "        real_example = {\n",
    "                \"image_id\": 1,\n",
    "                \"qa_id\": 10000001,\n",
    "                \"url\": first_example.get(\"image_url\", \"https://example.com/image.jpg\"),\n",
    "                \"question\": first_example[\"question\"],\n",
    "                \"pred_answer\": \"モデルによる予測回答例\",\n",
    "                \"gt_answer\": first_example[\"answer\"]\n",
    "        }\n",
    "        print(\"\\n実データを基にした出力例:\")\n",
    "        print(json.dumps(real_example, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XJNVSu7qIrT0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e0c4700d8f4b0598de4bcda49387d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddae26a722d4415bf0e26f4c6fe1a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/5.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3c1b8ea9f34ae3be17907a5c8bfdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['page_url', 'image_url', 'image', 'question', 'answer'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"SakanaAI/JA-VLM-Bench-In-the-Wild\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vJzHcQA4KEpb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-3-4b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599b7072da14ca481a853229c7be961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce269b635de24551a670496e69aa97af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Saved in gemma3_inference.jsonl\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import PIL\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "# ========== 1) データセット読み込み (decode=False) ==========\n",
    "\n",
    "dataset_name = \"SakanaAI/JA-VLM-Bench-In-the-Wild\"\n",
    "ds = load_dataset(dataset_name)\n",
    "ds = ds.cast_column(\"image\", Image(decode=False))  \n",
    "test_ds = ds[\"test\"]  \n",
    "\n",
    "# ========== 2) モデル/プロセッサの読み込み ==========\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "print(f\"Loading model: {model_id}\")\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ========== 3) map 用の推論関数 ==========\n",
    "\n",
    "def infer_on_example(example):\n",
    "    \"\"\"\n",
    "    「SakanaAI/JA-VLM-Bench-In-the-Wild」は\n",
    "      - page_url\n",
    "      - image_url\n",
    "      - image (dict: {'path': str, 'bytes': bytes})\n",
    "      - question\n",
    "      - answer\n",
    "    のカラムを持つ。\n",
    "\n",
    "    このexampleに対してモデル推論を行い、'pred_answer'を追加して返す。\n",
    "    \"\"\"\n",
    "\n",
    "    page_url = example[\"page_url\"]\n",
    "    image_url = example[\"image_url\"]\n",
    "    question_text = example[\"question\"]\n",
    "    gt_answer = example[\"answer\"]\n",
    "\n",
    "    img_bytes = example[\"image\"][\"bytes\"]  # decode=False なのでバイト列が入っている\n",
    "    temp_path = None\n",
    "    pred_answer = None\n",
    "\n",
    "    # 画像バイト列があれば一時ファイルに書き出してモデルに与える\n",
    "    if img_bytes is not None:\n",
    "        temp_path = \"temp_image.jpg\"\n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            f.write(img_bytes)\n",
    "\n",
    "        # 推論用プロンプト作成\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"You are a helpful Japanese vision-language assistant.\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": temp_path}, \n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"質問: {question_text}\\n\\nURL: {image_url}\\nページ: {page_url}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # プロンプトの形式をモデル向けに変換\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # 生成\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        gen_part = generated[0][input_len:]\n",
    "        decoded_answer = processor.decode(gen_part, skip_special_tokens=True)\n",
    "        pred_answer = decoded_answer\n",
    "\n",
    "        # 一時ファイル削除\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    # 推論結果を格納\n",
    "    example[\"pred_answer\"] = pred_answer\n",
    "\n",
    "    \n",
    "    return {k: v for k, v in example.items() if k != \"image\"}\n",
    "\n",
    "# ========== 4) map の実行 (推論) ==========\n",
    "\n",
    "test_inferred = test_ds.map(\n",
    "    infer_on_example,\n",
    "    batched=False,\n",
    "    remove_columns=[\"image\"]  # 明示的に削除しないと再エンコードエラーが出ることがある\n",
    ")\n",
    "\n",
    "# ========== 5) 結果書き出し ==========\n",
    "\n",
    "output_file = \"gemma3_inference.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in test_inferred:\n",
    "        # pred_answerはモデルの推論結果\n",
    "        item = {\n",
    "            \"page_url\": ex[\"page_url\"],\n",
    "            \"image_url\": ex[\"image_url\"],\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"gt_answer\": ex[\"answer\"],\n",
    "            \"pred_answer\": ex[\"pred_answer\"],\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"All done! Saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import PIL\n",
    "from datasets import load_dataset, Image\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "# ========== 1) Load dataset (decode=False) ==========\n",
    "\n",
    "dataset_name = \"SakanaAI/JA-VLM-Bench-In-the-Wild\"\n",
    "ds = load_dataset(dataset_name)\n",
    "ds = ds.cast_column(\"image\", Image(decode=False))  \n",
    "test_ds = ds[\"test\"]  \n",
    "\n",
    "# ========== 2) Load model/processor ==========\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "print(f\"Loading model: {model_id}\")\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ========== 3) Inference function for map ==========\n",
    "\n",
    "def infer_on_example(example):\n",
    "    \"\"\"\n",
    "    The \"SakanaAI/JA-VLM-Bench-In-the-Wild\" dataset has the following columns:\n",
    "      - page_url\n",
    "      - image_url\n",
    "      - image (dict: {'path': str, 'bytes': bytes})\n",
    "      - question\n",
    "      - answer\n",
    "    \n",
    "    This function performs model inference on the example and adds 'pred_answer' before returning.\n",
    "    \"\"\"\n",
    "\n",
    "    page_url = example[\"page_url\"]\n",
    "    image_url = example[\"image_url\"]\n",
    "    question_text = example[\"question\"]\n",
    "    gt_answer = example[\"answer\"]\n",
    "\n",
    "    img_bytes = example[\"image\"][\"bytes\"]  # Contains byte sequences because decode=False\n",
    "    temp_path = None\n",
    "    pred_answer = None\n",
    "\n",
    "    # If image bytes exist, write to a temporary file and provide to the model\n",
    "    if img_bytes is not None:\n",
    "        temp_path = \"temp_image.jpg\"\n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            f.write(img_bytes)\n",
    "\n",
    "        # Create prompt for inference\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"You are a helpful Japanese vision-language assistant.\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": temp_path}, \n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"質問: {question_text}\\n\\nURL: {image_url}\\nページ: {page_url}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Convert prompt format for the model\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        gen_part = generated[0][input_len:]\n",
    "        decoded_answer = processor.decode(gen_part, skip_special_tokens=True)\n",
    "        pred_answer = decoded_answer\n",
    "\n",
    "        # Delete temporary file\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "    # Store inference results\n",
    "    example[\"pred_answer\"] = pred_answer\n",
    "\n",
    "    \n",
    "    return {k: v for k, v in example.items() if k != \"image\"}\n",
    "\n",
    "# ========== 4) Execute map (inference) ==========\n",
    "\n",
    "test_inferred = test_ds.map(\n",
    "    infer_on_example,\n",
    "    batched=False,\n",
    "    remove_columns=[\"image\"]  # Explicitly removed to avoid re-encoding errors\n",
    ")\n",
    "\n",
    "# ========== 5) Write results ==========\n",
    "\n",
    "output_file = \"gemma3_inference.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in test_inferred:\n",
    "        # pred_answer is the model's inference result\n",
    "        item = {\n",
    "            \"page_url\": ex[\"page_url\"],\n",
    "            \"image_url\": ex[\"image_url\"],\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"gt_answer\": ex[\"answer\"],\n",
    "            \"pred_answer\": ex[\"pred_answer\"],\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"All done! Saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the inference results file\n",
    "\n",
    "# Translate the output and create a new file with English comments\n",
    "with open(\"gemma3_inference.jsonl\", \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "    open(\"gemma3_inference_en.jsonl\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "    \n",
    "    for line in f_in:\n",
    "       item = json.loads(line)\n",
    "       \n",
    "       # Create a new dictionary with translated field names\n",
    "       en_item = {\n",
    "          \"page_url\": item[\"page_url\"],\n",
    "          \"image_url\": item[\"image_url\"],\n",
    "          \"question\": item[\"question\"],  # Keep original Japanese question\n",
    "          \"ground_truth_answer\": item[\"gt_answer\"],  # Keep original Japanese answer\n",
    "          \"predicted_answer\": item[\"pred_answer\"],  # Keep original Japanese answer\n",
    "       }\n",
    "       \n",
    "       f_out.write(json.dumps(en_item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Translation complete! Saved in gemma3_inference_en.jsonl\")\n",
    "\n",
    "# You can also examine the first few records\n",
    "with open(\"gemma3_inference_en.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "       if i >= 3:  # Show first 3 examples\n",
    "          break\n",
    "       print(f\"Example {i+1}:\")\n",
    "       print(json.dumps(json.loads(line), ensure_ascii=False, indent=2))\n",
    "       print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2797c510aa534753a00ac978e721c18a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3dfab0ee3d3442d5b83d99bdbf548b45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48bda141b4ee4efb8314bd418dcf7995": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f436543f85a74d9daba22177ccebeda6",
      "placeholder": "​",
      "style": "IPY_MODEL_ae927396182045dc93da4bc888442c8e",
      "value": " 1/2 [00:22&lt;00:22, 22.11s/it]"
     }
    },
    "5aae52cd63ff4541ba193f2737cc1692": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8f87827437c4e0db0da258714a4c20d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6368dac1147b411bbe50d4039d5a8954",
      "value": 1
     }
    },
    "6368dac1147b411bbe50d4039d5a8954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae927396182045dc93da4bc888442c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8f87827437c4e0db0da258714a4c20d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0e2a9d0419d457bb2796191dae10629": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad932a9fb14468187b05d030b381d40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dafa848d595948269344813ae606019f",
       "IPY_MODEL_5aae52cd63ff4541ba193f2737cc1692",
       "IPY_MODEL_48bda141b4ee4efb8314bd418dcf7995"
      ],
      "layout": "IPY_MODEL_d0e2a9d0419d457bb2796191dae10629"
     }
    },
    "dafa848d595948269344813ae606019f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dfab0ee3d3442d5b83d99bdbf548b45",
      "placeholder": "​",
      "style": "IPY_MODEL_2797c510aa534753a00ac978e721c18a",
      "value": "Loading checkpoint shards:  50%"
     }
    },
    "f436543f85a74d9daba22177ccebeda6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
